{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_image_files(folder_path):\n",
    "    image_extensions = ['.jpeg','.jpg']\n",
    "\n",
    "    image_files = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, file)):\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_files.append(file)\n",
    "\n",
    "    return image_files\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "images=get_image_files('/home/coder/Abid/proj_with_marco/Data/train_data')\n",
    "with open('all_caps.json','r') as f:\n",
    "    caps=json.load(f)\n",
    "\n",
    "labels={}\n",
    "for k,v in zip(caps.keys(),caps.values()):\n",
    "    labels[k]=v[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p04_r00_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking into the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p12_r01_v03_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p10_r05_v13_c07_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking in the kitchen near a counter. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p09_r03_v07_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting down on the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p10_r04_v03_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is attempting to sit down on the green couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p06_r01_v10_c02_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is drinking from a bottle, sitting at a table. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p03_r00_v19_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on a couch watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p04_r02_v07_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is standing near the window, looking outside. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p25_r01_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on a green couch and watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p10_r11_v10_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is sitting down at a dining table. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p03_r05_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is standing in the living room, holding a bottle and drinking from it. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p12_r01_v06_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is drinking from a bottle in the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p19_r00_v03_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sleeping on a couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p20_r04_v15_c07_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is standing at the kitchen counter preparing food.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p03_r08_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is walking near the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p18_r01_v03_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying down on the couch, holding a baby. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p10_r03_v10_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is sitting at a table and holding a cup. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p14_r02_v10_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting at a table with a meal in front of them. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p04_r02_v04_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking a beverage from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p16_r00_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on the couch and waving their hands. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p14_r07_v15_c07_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is walking into a kitchen. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p18_r07_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle while sitting on the green couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p12_r03_v04_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on a couch, drinking a beverage from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p17_r05_v08_c02_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting down at the table. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p20_r03_v03_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is sitting down on the couch.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p09_r00_v02_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p03_r01_v13_c02_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking in the room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p14_r19_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is sitting down on the couch and watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p10_r01_v09_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting down at a dining table. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p06_r04_v10_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is reading a newspaper while drinking from a cup.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p15_r02_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on the couch in a relaxed position. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p19_r02_v07_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is examining a photo album or book while seated at a dining table. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p15_r05_v13_c07_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking towards the kitchen island. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p17_r12_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch and holding a book. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p06_r03_v10_c02_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person appears to be drinking from a glass. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p06_r03_v06_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is taking a drink from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p07_r01_v03_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on the couch while watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p14_r06_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying down on the couch with a remote control in their hand.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p07_r00_v03_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch and watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p04_r04_v04_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle in the living room.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p02_r01_v07_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch, holding a blanket over their shoulders and appearing to rest. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p04_r00_v01_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch, facing the TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p16_r02_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is entering the room <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p15_r19_v13_c06_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking through the kitchen and into the next room.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p25_r29_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking across the living room towards the seating area while holding an object. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p12_r01_v07_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking a beverage while watching TV in the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p17_r11_v04_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch reading a book. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p12_r01_v02_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a cup. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p20_r11_v13_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is walking towards the kitchen sink. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p09_r00_v06_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is lying down on a couch in the living room while watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p16_r00_v15_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is drinking from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p13_r00_v13_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle while standing. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p16_r01_v14_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking through the kitchen. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p03_r45_v04_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is standing by a couch, actively engaged in an activity. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p17_r05_v09_c02_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on a chair at a table and appears to be focused on using a laptop. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p10_r09_v10_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting at a table in a living room or kitchen area, holding an object in their hands. The table has items suggesting they might be eating or preparing a snack, such as a yellow cup and some small containers. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p13_r01_v13_c02_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p06_r01_v10_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p14_r01_v10_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting at a table, appearing to open a can. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p03_r02_v06_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying down on the green couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p03_r13_v03_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle while sitting on a green couch and watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p12_r18_v04_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking through the living room, moving from one area to another. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p03_r00_v12_c02_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting down on the couch, holding a laptop. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p15_r00_v04_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is standing near the TV in a living room while holding a cap. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p13_r00_v06_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle in the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p11_r00_v06_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking through a room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p06_r08_v13_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is walking towards the kitchen counter. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p19_r00_v04_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying down on a green couch while watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p25_r06_v10_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is sitting at a table, reading a magazine. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p04_r13_v04_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch and appears to be drinking from a bottle while watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p09_r01_v02_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on a couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p15_r01_v12_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting at a table, holding a phone to their ear. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p25_r02_v14_c07_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking through a modern kitchen. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p07_r02_v06_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch while watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p04_r06_v04_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle while standing in the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p15_r00_v05_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p06_r02_v06_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is taking a pill with water <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p14_r02_v07_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on a couch in a living room, appearing to engage in a conversation or listening to a phone call. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p20_r00_v04_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is lying on the couch and resting. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p18_r00_v06_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying down on the couch, appearing relaxed. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p03_r04_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p03_r07_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking in the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p15_r00_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking into the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p17_r02_v04_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch, reading a book. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p25_r00_v03_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on the couch with their feet up, watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p15_r00_v16_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle while standing in the kitchen.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p10_r06_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the green couch in the living room, appearing to be engaged with something on the coffee table. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p17_r05_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on a couch watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p07_r04_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting down on the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p15_r12_v17_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is walking in the kitchen. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p11_r02_v02_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on a couch with their legs stretched out and hands resting on the arm. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p15_r00_v11_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p06_r00_v06_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch, holding a bottle while watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p25_r03_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying down on the couch while watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p25_r04_v06_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is seated on a couch in the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p12_r14_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking across the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p19_r04_v09_c02_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is walking into the kitchen area. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p11_r01_v09_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is seated at a table using a laptop.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p14_r00_v06_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is watching TV and holding a cup, likely containing a beverage.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p20_r00_v04_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying down on the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p25_r00_v03_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on the couch, appearing to be relaxing or possibly sleeping. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p18_r01_v06_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on a green couch using a remote control. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p16_r01_v15_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle in the kitchen. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p18_r02_v05_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting down on the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p19_r12_v03_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking into the room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p25_r24_v04_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking across the room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p03_r06_v03_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is watching TV on the couch while holding a drink. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p25_r03_v03_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is lying down on the couch in the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p02_r02_v07_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying down on the couch, covered with a blanket, watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p12_r03_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is drinking a beverage from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p16_r36_v12_c06_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is walking out of the kitchen area <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p10_r11_v12_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking in the kitchen while holding a bowl of vegetables. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p03_r01_v03_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on a couch and drinking from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p14_r03_v02_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch and appears to be focusing on a laptop.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p14_r02_v06_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking a beverage from a bottle while watching TV. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p04_r10_v04_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is taking a drink from a bottle. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p15_r01_v02_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying down on the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p17_r00_v12_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking through the kitchen. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p16_r03_v07_c01_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting at a table in a kitchen, looking at a laptop. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p04_r05_v03_c05_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting down on a chair in the living room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p03_r02_v05_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'the person is drinking from a bottle while sitting on the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p20_r04_v07_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting on the couch and holding a cup. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p14_r00_v13_c02_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle while seated at the kitchen island. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p06_r00_v05_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking in the room. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p16_r28_v12_c06_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking towards the kitchen counter. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Sitdown_p14_r05_v02_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is sitting down on the couch and placing or reading a book.\\n <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Laydown_p10_r02_v03_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is lying on the couch, covered with a blanket, resting or sleeping. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p09_r00_v07_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is drinking from a bottle while sitting on the couch. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Drink.Frombottle_p04_r00_v07_c04_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is holding a small object near their mouth, possibly drinking or communicating with someone. <|end_of_text|>'),\n",
       " ('/home/coder/Abid/proj_with_marco/Data/train_data/Walk_p25_r04_v14_c03_middle.jpg',\n",
       "  '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a vision assistant, given an image you describe it briefly.<|eot_id|><|start_header_id|>user<|end_header_id|><|image|>What is the person doing in this image?<|eot_id|><|start_header_id|>assistant<|end_header_id|>description:',\n",
       "  'The person is walking in the kitchen. <|end_of_text|>')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[(\"/home/coder/Abid/proj_with_marco/Data/train_data/\"+img,prompt,caps[img.split('_m',1)[0].replace('.','_')][-1]+\" <|end_of_text|>\") for img in images[:]]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/miniconda3/envs/gouga/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 27.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MllamaForConditionalGeneration(\n",
       "      (vision_model): MllamaVisionModel(\n",
       "        (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), padding=valid, bias=False)\n",
       "        (gated_positional_embedding): MllamaPrecomputedPositionEmbedding(\n",
       "          (tile_embedding): Embedding(9, 8197120)\n",
       "        )\n",
       "        (pre_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
       "          (embedding): Embedding(9, 5120)\n",
       "        )\n",
       "        (post_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
       "          (embedding): Embedding(9, 5120)\n",
       "        )\n",
       "        (layernorm_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (transformer): MllamaVisionEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x MllamaVisionEncoderLayer(\n",
       "              (self_attn): MllamaVisionSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaVisionMLP(\n",
       "                (activation_fn): GELUActivation()\n",
       "                (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "              (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (global_transformer): MllamaVisionEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-7): 8 x MllamaVisionEncoderLayer(\n",
       "              (self_attn): MllamaVisionSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaVisionMLP(\n",
       "                (activation_fn): GELUActivation()\n",
       "                (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "                (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "              (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (language_model): MllamaForCausalLM(\n",
       "        (model): MllamaTextModel(\n",
       "          (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
       "              (self_attn): MllamaTextSelfSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (3): MllamaCrossAttentionDecoderLayer(\n",
       "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "              (self_attn): MllamaTextSelfSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (8): MllamaCrossAttentionDecoderLayer(\n",
       "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "              (self_attn): MllamaTextSelfSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (13): MllamaCrossAttentionDecoderLayer(\n",
       "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "              (self_attn): MllamaTextSelfSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (18): MllamaCrossAttentionDecoderLayer(\n",
       "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "              (self_attn): MllamaTextSelfSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (23): MllamaCrossAttentionDecoderLayer(\n",
       "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "              (self_attn): MllamaTextSelfSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (28): MllamaCrossAttentionDecoderLayer(\n",
       "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "              (self_attn): MllamaTextSelfSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (33): MllamaCrossAttentionDecoderLayer(\n",
       "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "              (self_attn): MllamaTextSelfSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (38): MllamaCrossAttentionDecoderLayer(\n",
       "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "            (39): MllamaSelfAttentionDecoderLayer(\n",
       "              (self_attn): MllamaTextSelfSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MllamaTextMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (rotary_emb): MllamaRotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "      )\n",
       "      (multi_modal_projector): Linear(in_features=7680, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.optim import AdamW  # ✅ Correct and recommended\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "# Check if CUDA is available\n",
    "from transformers import  AutoTokenizer\n",
    "\n",
    "\n",
    "# device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# === LoRA setup ===\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# === Configuration ===\n",
    "epochs = 7\n",
    "lr = 5e-5\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "class FTDataset(Dataset):\n",
    "    def __init__(self, data_list, processor):\n",
    "        \"\"\"\n",
    "        data_list: list of (image_path, prompt, label)\n",
    "        processor: Hugging Face processor (tokenizer + image preprocessor)\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, prompt, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        inputs = self.processor(text=prompt, images=image, return_tensors=\"pt\", padding=True)\n",
    "        label_ids = self.processor.tokenizer(label, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"label_ids\": label_ids\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# === Init Dataloader ===\n",
    "dataset = FTDataset(data,processor=processor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 7.0675\n",
      "Epoch 1 - Loss: 6.5481\n",
      "Epoch 1 - Loss: 6.1351\n",
      "Epoch 1 - Loss: 6.1375\n",
      "Epoch 1 - Loss: 5.9309\n",
      "Epoch 1 - Loss: 4.9899\n",
      "Epoch 1 - Loss: 5.6071\n",
      "Epoch 1 - Loss: 5.2417\n",
      "Epoch 1 - Loss: 5.1329\n",
      "Epoch 1 - Loss: 5.2329\n",
      "Epoch 1 - Loss: 4.3001\n",
      "Epoch 1 - Loss: 5.0009\n",
      "Epoch 1 - Loss: 5.7564\n",
      "Epoch 1 - Loss: 5.1470\n",
      "Epoch 1 - Loss: 6.1195\n",
      "Epoch 1 - Loss: 4.5412\n",
      "Epoch 1 - Loss: 3.9975\n",
      "Epoch 1 - Loss: 3.3926\n",
      "Epoch 1 - Loss: 4.5545\n",
      "Epoch 1 - Loss: 4.5010\n",
      "Epoch 1 - Loss: 4.5449\n",
      "Epoch 1 - Loss: 3.9828\n",
      "Epoch 1 - Loss: 3.1715\n",
      "Epoch 1 - Loss: 4.0016\n",
      "Epoch 1 - Loss: 3.8669\n",
      "Epoch 1 - Loss: 3.5682\n",
      "Epoch 1 - Loss: 3.6800\n",
      "Epoch 1 - Loss: 2.7044\n",
      "Epoch 1 - Loss: 3.0204\n",
      "Epoch 1 - Loss: 3.2493\n",
      "Epoch 1 - Loss: 2.3105\n",
      "Epoch 1 - Loss: 3.2381\n",
      "Epoch 1 - Loss: 2.8517\n",
      "Epoch 1 - Loss: 1.8631\n",
      "Epoch 1 - Loss: 2.2381\n",
      "Epoch 1 - Loss: 2.0867\n",
      "Epoch 1 - Loss: 1.8958\n",
      "Epoch 1 - Loss: 1.8540\n",
      "Epoch 1 - Loss: 1.1345\n",
      "Epoch 1 - Loss: 1.9422\n",
      "Epoch 1 - Loss: 1.7818\n",
      "Epoch 1 - Loss: 1.0328\n",
      "Epoch 1 - Loss: 2.5548\n",
      "Epoch 1 - Loss: 1.6991\n",
      "Epoch 1 - Loss: 1.9968\n",
      "Epoch 1 - Loss: 1.4520\n",
      "Epoch 1 - Loss: 0.7605\n",
      "Epoch 1 - Loss: 0.9435\n",
      "Epoch 1 - Loss: 1.3579\n",
      "Epoch 1 - Loss: 1.3296\n",
      "Epoch 1 - Loss: 1.2638\n",
      "Epoch 1 - Loss: 1.3611\n",
      "Epoch 1 - Loss: 1.0189\n",
      "Epoch 1 - Loss: 2.0054\n",
      "Epoch 1 - Loss: 1.6208\n",
      "Epoch 1 - Loss: 2.1823\n",
      "Epoch 1 - Loss: 0.6889\n",
      "Epoch 1 - Loss: 0.6686\n",
      "Epoch 1 - Loss: 1.6752\n",
      "Epoch 1 - Loss: 1.5496\n",
      "Epoch 1 - Loss: 1.2572\n",
      "Epoch 1 - Loss: 1.2868\n",
      "Epoch 1 - Loss: 1.0960\n",
      "Epoch 1 - Loss: 1.9344\n",
      "Epoch 1 - Loss: 0.7840\n",
      "Epoch 1 - Loss: 1.1294\n",
      "Epoch 1 - Loss: 1.2264\n",
      "Epoch 1 - Loss: 1.2224\n",
      "Epoch 1 - Loss: 1.2256\n",
      "Epoch 1 - Loss: 0.9093\n",
      "Epoch 1 - Loss: 0.9560\n",
      "Epoch 1 - Loss: 0.9169\n",
      "Epoch 1 - Loss: 1.5194\n",
      "Epoch 1 - Loss: 0.6562\n",
      "Epoch 1 - Loss: 0.8562\n",
      "Epoch 1 - Loss: 0.6727\n",
      "Epoch 1 - Loss: 2.5420\n",
      "Epoch 1 - Loss: 1.3580\n",
      "Epoch 1 - Loss: 1.5079\n",
      "Epoch 1 - Loss: 0.9783\n",
      "Epoch 1 - Loss: 0.5133\n",
      "Epoch 1 - Loss: 0.8057\n",
      "Epoch 1 - Loss: 0.7765\n",
      "Epoch 1 - Loss: 0.6441\n",
      "Epoch 1 - Loss: 1.0280\n",
      "Epoch 1 - Loss: 1.0209\n",
      "Epoch 1 - Loss: 1.3856\n",
      "Epoch 1 - Loss: 0.7864\n",
      "Epoch 1 - Loss: 0.8535\n",
      "Epoch 1 - Loss: 1.0659\n",
      "Epoch 1 - Loss: 1.3295\n",
      "Epoch 1 - Loss: 0.4427\n",
      "Epoch 1 - Loss: 0.9187\n",
      "Epoch 1 - Loss: 0.5705\n",
      "Epoch 1 - Loss: 0.9421\n",
      "Epoch 1 - Loss: 0.9025\n",
      "Epoch 1 - Loss: 1.2419\n",
      "Epoch 1 - Loss: 1.5948\n",
      "Epoch 1 - Loss: 1.9671\n",
      "Epoch 1 - Loss: 0.5146\n",
      "Epoch 1 - Loss: 1.0021\n",
      "Epoch 1 - Loss: 1.6146\n",
      "Epoch 1 - Loss: 0.7276\n",
      "Epoch 1 - Loss: 0.6555\n",
      "Epoch 1 - Loss: 0.7757\n",
      "Epoch 1 - Loss: 0.7821\n",
      "Epoch 1 - Loss: 1.0970\n",
      "Epoch 1 - Loss: 0.7109\n",
      "Epoch 1 - Loss: 0.8994\n",
      "Epoch 1 - Loss: 1.0150\n",
      "Epoch 1 - Loss: 1.5095\n",
      "Epoch 1 - Loss: 1.6023\n",
      "Epoch 1 - Loss: 0.7451\n",
      "Epoch 1 - Loss: 1.3752\n",
      "Epoch 1 - Loss: 0.8849\n",
      "Epoch 1 - Loss: 0.6919\n",
      "Epoch 1 - Loss: 0.4913\n",
      "Epoch 1 - Loss: 0.6367\n",
      "Epoch 1 - Loss: 1.0983\n",
      "Epoch 1 - Loss: 0.8120\n",
      "Epoch 1 - Loss: 0.6737\n",
      "Epoch 1 - Loss: 0.7610\n",
      "Epoch 1 - Loss: 1.0591\n",
      "Epoch 1 - Loss: 0.8440\n",
      "Epoch 1 - Loss: 0.7310\n",
      "Epoch 1 - Loss: 1.7559\n",
      "Epoch 1 - Loss: 1.2534\n",
      "Epoch 1 - Loss: 0.4036\n",
      "Epoch 1 - Loss: 1.9406\n",
      "Epoch 1 - Loss: 0.5780\n",
      "Epoch 2 - Loss: 0.5389\n",
      "Epoch 2 - Loss: 0.4924\n",
      "Epoch 2 - Loss: 1.0921\n",
      "Epoch 2 - Loss: 0.5074\n",
      "Epoch 2 - Loss: 1.5418\n",
      "Epoch 2 - Loss: 1.0261\n",
      "Epoch 2 - Loss: 0.9221\n",
      "Epoch 2 - Loss: 1.1240\n",
      "Epoch 2 - Loss: 1.0089\n",
      "Epoch 2 - Loss: 1.0339\n",
      "Epoch 2 - Loss: 0.8505\n",
      "Epoch 2 - Loss: 0.7676\n",
      "Epoch 2 - Loss: 1.0336\n",
      "Epoch 2 - Loss: 1.1982\n",
      "Epoch 2 - Loss: 1.2327\n",
      "Epoch 2 - Loss: 1.0148\n",
      "Epoch 2 - Loss: 0.9463\n",
      "Epoch 2 - Loss: 0.6996\n",
      "Epoch 2 - Loss: 0.5681\n",
      "Epoch 2 - Loss: 1.4419\n",
      "Epoch 2 - Loss: 1.1759\n",
      "Epoch 2 - Loss: 0.8355\n",
      "Epoch 2 - Loss: 0.6481\n",
      "Epoch 2 - Loss: 0.7572\n",
      "Epoch 2 - Loss: 1.1100\n",
      "Epoch 2 - Loss: 0.4977\n",
      "Epoch 2 - Loss: 0.6497\n",
      "Epoch 2 - Loss: 0.7846\n",
      "Epoch 2 - Loss: 0.6359\n",
      "Epoch 2 - Loss: 1.3041\n",
      "Epoch 2 - Loss: 0.7269\n",
      "Epoch 2 - Loss: 1.9507\n",
      "Epoch 2 - Loss: 0.8014\n",
      "Epoch 2 - Loss: 0.5801\n",
      "Epoch 2 - Loss: 1.0341\n",
      "Epoch 2 - Loss: 0.5419\n",
      "Epoch 2 - Loss: 0.5933\n",
      "Epoch 2 - Loss: 0.7517\n",
      "Epoch 2 - Loss: 0.3900\n",
      "Epoch 2 - Loss: 0.5714\n",
      "Epoch 2 - Loss: 1.2436\n",
      "Epoch 2 - Loss: 0.6968\n",
      "Epoch 2 - Loss: 2.0000\n",
      "Epoch 2 - Loss: 1.2360\n",
      "Epoch 2 - Loss: 1.3676\n",
      "Epoch 2 - Loss: 0.8131\n",
      "Epoch 2 - Loss: 0.5151\n",
      "Epoch 2 - Loss: 0.6268\n",
      "Epoch 2 - Loss: 1.0212\n",
      "Epoch 2 - Loss: 0.8514\n",
      "Epoch 2 - Loss: 0.6963\n",
      "Epoch 2 - Loss: 0.8943\n",
      "Epoch 2 - Loss: 0.6031\n",
      "Epoch 2 - Loss: 1.6291\n",
      "Epoch 2 - Loss: 1.1920\n",
      "Epoch 2 - Loss: 1.9028\n",
      "Epoch 2 - Loss: 0.3294\n",
      "Epoch 2 - Loss: 0.3182\n",
      "Epoch 2 - Loss: 1.1694\n",
      "Epoch 2 - Loss: 0.8163\n",
      "Epoch 2 - Loss: 0.8865\n",
      "Epoch 2 - Loss: 0.8813\n",
      "Epoch 2 - Loss: 0.7715\n",
      "Epoch 2 - Loss: 1.6588\n",
      "Epoch 2 - Loss: 0.3914\n",
      "Epoch 2 - Loss: 0.6475\n",
      "Epoch 2 - Loss: 0.8213\n",
      "Epoch 2 - Loss: 0.6927\n",
      "Epoch 2 - Loss: 1.0685\n",
      "Epoch 2 - Loss: 0.6142\n",
      "Epoch 2 - Loss: 0.6693\n",
      "Epoch 2 - Loss: 0.7371\n",
      "Epoch 2 - Loss: 0.9884\n",
      "Epoch 2 - Loss: 0.4840\n",
      "Epoch 2 - Loss: 0.4832\n",
      "Epoch 2 - Loss: 0.3223\n",
      "Epoch 2 - Loss: 2.0331\n",
      "Epoch 2 - Loss: 1.0423\n",
      "Epoch 2 - Loss: 1.1581\n",
      "Epoch 2 - Loss: 0.5746\n",
      "Epoch 2 - Loss: 0.3193\n",
      "Epoch 2 - Loss: 0.4582\n",
      "Epoch 2 - Loss: 0.5352\n",
      "Epoch 2 - Loss: 0.4568\n",
      "Epoch 2 - Loss: 0.7721\n",
      "Epoch 2 - Loss: 0.6696\n",
      "Epoch 2 - Loss: 0.8883\n",
      "Epoch 2 - Loss: 0.5412\n",
      "Epoch 2 - Loss: 0.6066\n",
      "Epoch 2 - Loss: 0.7163\n",
      "Epoch 2 - Loss: 1.0456\n",
      "Epoch 2 - Loss: 0.2337\n",
      "Epoch 2 - Loss: 0.6370\n",
      "Epoch 2 - Loss: 0.3904\n",
      "Epoch 2 - Loss: 0.7395\n",
      "Epoch 2 - Loss: 0.5648\n",
      "Epoch 2 - Loss: 0.9143\n",
      "Epoch 2 - Loss: 1.2717\n",
      "Epoch 2 - Loss: 1.6019\n",
      "Epoch 2 - Loss: 0.3381\n",
      "Epoch 2 - Loss: 0.8169\n",
      "Epoch 2 - Loss: 1.1228\n",
      "Epoch 2 - Loss: 0.4569\n",
      "Epoch 2 - Loss: 0.3902\n",
      "Epoch 2 - Loss: 0.5225\n",
      "Epoch 2 - Loss: 0.5073\n",
      "Epoch 2 - Loss: 0.8020\n",
      "Epoch 2 - Loss: 0.5230\n",
      "Epoch 2 - Loss: 0.7106\n",
      "Epoch 2 - Loss: 0.7777\n",
      "Epoch 2 - Loss: 1.1201\n",
      "Epoch 2 - Loss: 1.5130\n",
      "Epoch 2 - Loss: 0.6894\n",
      "Epoch 2 - Loss: 1.0952\n",
      "Epoch 2 - Loss: 0.6409\n",
      "Epoch 2 - Loss: 0.4644\n",
      "Epoch 2 - Loss: 0.3945\n",
      "Epoch 2 - Loss: 0.4973\n",
      "Epoch 2 - Loss: 0.9181\n",
      "Epoch 2 - Loss: 0.6480\n",
      "Epoch 2 - Loss: 0.4403\n",
      "Epoch 2 - Loss: 0.5948\n",
      "Epoch 2 - Loss: 0.7790\n",
      "Epoch 2 - Loss: 0.6509\n",
      "Epoch 2 - Loss: 0.5328\n",
      "Epoch 2 - Loss: 1.4945\n",
      "Epoch 2 - Loss: 0.9053\n",
      "Epoch 2 - Loss: 0.3223\n",
      "Epoch 2 - Loss: 1.5590\n",
      "Epoch 2 - Loss: 0.4395\n",
      "Epoch 3 - Loss: 0.4979\n",
      "Epoch 3 - Loss: 0.4437\n",
      "Epoch 3 - Loss: 0.9065\n",
      "Epoch 3 - Loss: 0.4024\n",
      "Epoch 3 - Loss: 1.1674\n",
      "Epoch 3 - Loss: 0.8877\n",
      "Epoch 3 - Loss: 0.7871\n",
      "Epoch 3 - Loss: 0.9763\n",
      "Epoch 3 - Loss: 0.7578\n",
      "Epoch 3 - Loss: 0.8296\n",
      "Epoch 3 - Loss: 0.7741\n",
      "Epoch 3 - Loss: 0.5667\n",
      "Epoch 3 - Loss: 0.8372\n",
      "Epoch 3 - Loss: 1.0491\n",
      "Epoch 3 - Loss: 1.0059\n",
      "Epoch 3 - Loss: 0.9264\n",
      "Epoch 3 - Loss: 0.6653\n",
      "Epoch 3 - Loss: 0.6357\n",
      "Epoch 3 - Loss: 0.4578\n",
      "Epoch 3 - Loss: 1.2453\n",
      "Epoch 3 - Loss: 0.9322\n",
      "Epoch 3 - Loss: 0.6636\n",
      "Epoch 3 - Loss: 0.6089\n",
      "Epoch 3 - Loss: 0.7486\n",
      "Epoch 3 - Loss: 0.8242\n",
      "Epoch 3 - Loss: 0.4529\n",
      "Epoch 3 - Loss: 0.5260\n",
      "Epoch 3 - Loss: 0.5700\n",
      "Epoch 3 - Loss: 0.5537\n",
      "Epoch 3 - Loss: 1.1731\n",
      "Epoch 3 - Loss: 0.6538\n",
      "Epoch 3 - Loss: 1.5837\n",
      "Epoch 3 - Loss: 0.6092\n",
      "Epoch 3 - Loss: 0.5729\n",
      "Epoch 3 - Loss: 1.1333\n",
      "Epoch 3 - Loss: 0.4362\n",
      "Epoch 3 - Loss: 0.5512\n",
      "Epoch 3 - Loss: 0.7503\n",
      "Epoch 3 - Loss: 0.4229\n",
      "Epoch 3 - Loss: 0.5100\n",
      "Epoch 3 - Loss: 1.1507\n",
      "Epoch 3 - Loss: 0.6875\n",
      "Epoch 3 - Loss: 1.5459\n",
      "Epoch 3 - Loss: 1.1277\n",
      "Epoch 3 - Loss: 1.2781\n",
      "Epoch 3 - Loss: 0.6655\n",
      "Epoch 3 - Loss: 0.5288\n",
      "Epoch 3 - Loss: 0.6066\n",
      "Epoch 3 - Loss: 0.8885\n",
      "Epoch 3 - Loss: 0.7479\n",
      "Epoch 3 - Loss: 0.5648\n",
      "Epoch 3 - Loss: 0.8498\n",
      "Epoch 3 - Loss: 0.5820\n",
      "Epoch 3 - Loss: 1.4814\n",
      "Epoch 3 - Loss: 1.0856\n",
      "Epoch 3 - Loss: 1.8229\n",
      "Epoch 3 - Loss: 0.3638\n",
      "Epoch 3 - Loss: 0.3568\n",
      "Epoch 3 - Loss: 1.1063\n",
      "Epoch 3 - Loss: 0.6845\n",
      "Epoch 3 - Loss: 0.7576\n",
      "Epoch 3 - Loss: 0.7747\n",
      "Epoch 3 - Loss: 0.7123\n",
      "Epoch 3 - Loss: 1.5727\n",
      "Epoch 3 - Loss: 0.3584\n",
      "Epoch 3 - Loss: 0.6302\n",
      "Epoch 3 - Loss: 0.6668\n",
      "Epoch 3 - Loss: 0.6135\n",
      "Epoch 3 - Loss: 0.9582\n",
      "Epoch 3 - Loss: 0.5865\n",
      "Epoch 3 - Loss: 0.7032\n",
      "Epoch 3 - Loss: 0.6969\n",
      "Epoch 3 - Loss: 0.8525\n",
      "Epoch 3 - Loss: 0.5008\n",
      "Epoch 3 - Loss: 0.4351\n",
      "Epoch 3 - Loss: 0.2775\n",
      "Epoch 3 - Loss: 1.6506\n",
      "Epoch 3 - Loss: 0.9714\n",
      "Epoch 3 - Loss: 0.9981\n",
      "Epoch 3 - Loss: 0.5679\n",
      "Epoch 3 - Loss: 0.2788\n",
      "Epoch 3 - Loss: 0.4007\n",
      "Epoch 3 - Loss: 0.4941\n",
      "Epoch 3 - Loss: 0.4554\n",
      "Epoch 3 - Loss: 0.7177\n",
      "Epoch 3 - Loss: 0.5778\n",
      "Epoch 3 - Loss: 0.7503\n",
      "Epoch 3 - Loss: 0.5192\n",
      "Epoch 3 - Loss: 0.5353\n",
      "Epoch 3 - Loss: 0.6915\n",
      "Epoch 3 - Loss: 0.8490\n",
      "Epoch 3 - Loss: 0.2478\n",
      "Epoch 3 - Loss: 0.6138\n",
      "Epoch 3 - Loss: 0.3577\n",
      "Epoch 3 - Loss: 0.7405\n",
      "Epoch 3 - Loss: 0.5324\n",
      "Epoch 3 - Loss: 0.8833\n",
      "Epoch 3 - Loss: 1.1475\n",
      "Epoch 3 - Loss: 1.3538\n",
      "Epoch 3 - Loss: 0.3501\n",
      "Epoch 3 - Loss: 0.7516\n",
      "Epoch 3 - Loss: 0.9757\n",
      "Epoch 3 - Loss: 0.4460\n",
      "Epoch 3 - Loss: 0.3649\n",
      "Epoch 3 - Loss: 0.5199\n",
      "Epoch 3 - Loss: 0.4728\n",
      "Epoch 3 - Loss: 0.7797\n",
      "Epoch 3 - Loss: 0.4743\n",
      "Epoch 3 - Loss: 0.5824\n",
      "Epoch 3 - Loss: 0.7410\n",
      "Epoch 3 - Loss: 0.9129\n",
      "Epoch 3 - Loss: 1.4392\n",
      "Epoch 3 - Loss: 0.7175\n",
      "Epoch 3 - Loss: 0.9861\n",
      "Epoch 3 - Loss: 0.6019\n",
      "Epoch 3 - Loss: 0.4449\n",
      "Epoch 3 - Loss: 0.3671\n",
      "Epoch 3 - Loss: 0.4705\n",
      "Epoch 3 - Loss: 0.8800\n",
      "Epoch 3 - Loss: 0.6154\n",
      "Epoch 3 - Loss: 0.4367\n",
      "Epoch 3 - Loss: 0.5815\n",
      "Epoch 3 - Loss: 0.7128\n",
      "Epoch 3 - Loss: 0.6133\n",
      "Epoch 3 - Loss: 0.4963\n",
      "Epoch 3 - Loss: 1.3299\n",
      "Epoch 3 - Loss: 0.7446\n",
      "Epoch 3 - Loss: 0.3238\n",
      "Epoch 3 - Loss: 1.3699\n",
      "Epoch 3 - Loss: 0.4242\n",
      "Epoch 4 - Loss: 0.4917\n",
      "Epoch 4 - Loss: 0.4564\n",
      "Epoch 4 - Loss: 0.8146\n",
      "Epoch 4 - Loss: 0.3992\n",
      "Epoch 4 - Loss: 1.0663\n",
      "Epoch 4 - Loss: 0.8386\n",
      "Epoch 4 - Loss: 0.7605\n",
      "Epoch 4 - Loss: 0.8587\n",
      "Epoch 4 - Loss: 0.6722\n",
      "Epoch 4 - Loss: 0.7702\n",
      "Epoch 4 - Loss: 0.7422\n",
      "Epoch 4 - Loss: 0.5608\n",
      "Epoch 4 - Loss: 0.8010\n",
      "Epoch 4 - Loss: 0.9993\n",
      "Epoch 4 - Loss: 0.9357\n",
      "Epoch 4 - Loss: 0.8255\n",
      "Epoch 4 - Loss: 0.6438\n",
      "Epoch 4 - Loss: 0.6050\n",
      "Epoch 4 - Loss: 0.4193\n",
      "Epoch 4 - Loss: 1.0982\n",
      "Epoch 4 - Loss: 0.9448\n",
      "Epoch 4 - Loss: 0.5693\n",
      "Epoch 4 - Loss: 0.6077\n",
      "Epoch 4 - Loss: 0.7038\n",
      "Epoch 4 - Loss: 0.7873\n",
      "Epoch 4 - Loss: 0.4434\n",
      "Epoch 4 - Loss: 0.4840\n",
      "Epoch 4 - Loss: 0.5552\n",
      "Epoch 4 - Loss: 0.4931\n",
      "Epoch 4 - Loss: 1.0860\n",
      "Epoch 4 - Loss: 0.6244\n",
      "Epoch 4 - Loss: 1.3265\n",
      "Epoch 4 - Loss: 0.5820\n",
      "Epoch 4 - Loss: 0.5488\n",
      "Epoch 4 - Loss: 1.1079\n",
      "Epoch 4 - Loss: 0.4101\n",
      "Epoch 4 - Loss: 0.5133\n",
      "Epoch 4 - Loss: 0.7076\n",
      "Epoch 4 - Loss: 0.4173\n",
      "Epoch 4 - Loss: 0.4865\n",
      "Epoch 4 - Loss: 1.0196\n",
      "Epoch 4 - Loss: 0.6961\n",
      "Epoch 4 - Loss: 1.3854\n",
      "Epoch 4 - Loss: 1.0221\n",
      "Epoch 4 - Loss: 1.2273\n",
      "Epoch 4 - Loss: 0.6325\n",
      "Epoch 4 - Loss: 0.5187\n",
      "Epoch 4 - Loss: 0.5908\n",
      "Epoch 4 - Loss: 0.8359\n",
      "Epoch 4 - Loss: 0.7299\n",
      "Epoch 4 - Loss: 0.5497\n",
      "Epoch 4 - Loss: 0.8651\n",
      "Epoch 4 - Loss: 0.5752\n",
      "Epoch 4 - Loss: 1.3542\n",
      "Epoch 4 - Loss: 1.0132\n",
      "Epoch 4 - Loss: 1.7590\n",
      "Epoch 4 - Loss: 0.3653\n",
      "Epoch 4 - Loss: 0.3518\n",
      "Epoch 4 - Loss: 1.0705\n",
      "Epoch 4 - Loss: 0.6611\n",
      "Epoch 4 - Loss: 0.6929\n",
      "Epoch 4 - Loss: 0.6864\n",
      "Epoch 4 - Loss: 0.6764\n",
      "Epoch 4 - Loss: 1.4565\n",
      "Epoch 4 - Loss: 0.3463\n",
      "Epoch 4 - Loss: 0.6215\n",
      "Epoch 4 - Loss: 0.6452\n",
      "Epoch 4 - Loss: 0.5634\n",
      "Epoch 4 - Loss: 0.9076\n",
      "Epoch 4 - Loss: 0.5566\n",
      "Epoch 4 - Loss: 0.6894\n",
      "Epoch 4 - Loss: 0.6366\n",
      "Epoch 4 - Loss: 0.7006\n",
      "Epoch 4 - Loss: 0.5079\n",
      "Epoch 4 - Loss: 0.4134\n",
      "Epoch 4 - Loss: 0.2664\n",
      "Epoch 4 - Loss: 1.3041\n",
      "Epoch 4 - Loss: 0.8886\n",
      "Epoch 4 - Loss: 0.9376\n",
      "Epoch 4 - Loss: 0.5978\n",
      "Epoch 4 - Loss: 0.2526\n",
      "Epoch 4 - Loss: 0.3798\n",
      "Epoch 4 - Loss: 0.4723\n",
      "Epoch 4 - Loss: 0.4586\n",
      "Epoch 4 - Loss: 0.6874\n",
      "Epoch 4 - Loss: 0.5458\n",
      "Epoch 4 - Loss: 0.6701\n",
      "Epoch 4 - Loss: 0.5426\n",
      "Epoch 4 - Loss: 0.4866\n",
      "Epoch 4 - Loss: 0.6597\n",
      "Epoch 4 - Loss: 0.7190\n",
      "Epoch 4 - Loss: 0.2357\n",
      "Epoch 4 - Loss: 0.6129\n",
      "Epoch 4 - Loss: 0.3396\n",
      "Epoch 4 - Loss: 0.7661\n",
      "Epoch 4 - Loss: 0.4973\n",
      "Epoch 4 - Loss: 0.8791\n",
      "Epoch 4 - Loss: 1.0748\n",
      "Epoch 4 - Loss: 1.2167\n",
      "Epoch 4 - Loss: 0.3386\n",
      "Epoch 4 - Loss: 0.6899\n",
      "Epoch 4 - Loss: 0.8763\n",
      "Epoch 4 - Loss: 0.4743\n",
      "Epoch 4 - Loss: 0.3423\n",
      "Epoch 4 - Loss: 0.5105\n",
      "Epoch 4 - Loss: 0.4677\n",
      "Epoch 4 - Loss: 0.7535\n",
      "Epoch 4 - Loss: 0.4621\n",
      "Epoch 4 - Loss: 0.5334\n",
      "Epoch 4 - Loss: 0.7390\n",
      "Epoch 4 - Loss: 0.7952\n",
      "Epoch 4 - Loss: 1.2460\n",
      "Epoch 4 - Loss: 0.7251\n",
      "Epoch 4 - Loss: 0.8941\n",
      "Epoch 4 - Loss: 0.5937\n",
      "Epoch 4 - Loss: 0.4437\n",
      "Epoch 4 - Loss: 0.3567\n",
      "Epoch 4 - Loss: 0.4569\n",
      "Epoch 4 - Loss: 0.8220\n",
      "Epoch 4 - Loss: 0.5993\n",
      "Epoch 4 - Loss: 0.4578\n",
      "Epoch 4 - Loss: 0.5904\n",
      "Epoch 4 - Loss: 0.6392\n",
      "Epoch 4 - Loss: 0.5707\n",
      "Epoch 4 - Loss: 0.4715\n",
      "Epoch 4 - Loss: 1.1402\n",
      "Epoch 4 - Loss: 0.6462\n",
      "Epoch 4 - Loss: 0.3306\n",
      "Epoch 4 - Loss: 1.2262\n",
      "Epoch 4 - Loss: 0.4153\n",
      "Epoch 5 - Loss: 0.4962\n",
      "Epoch 5 - Loss: 0.4687\n",
      "Epoch 5 - Loss: 0.7279\n",
      "Epoch 5 - Loss: 0.3697\n",
      "Epoch 5 - Loss: 0.9857\n",
      "Epoch 5 - Loss: 0.7658\n",
      "Epoch 5 - Loss: 0.7341\n",
      "Epoch 5 - Loss: 0.7810\n",
      "Epoch 5 - Loss: 0.6184\n",
      "Epoch 5 - Loss: 0.7250\n",
      "Epoch 5 - Loss: 0.7173\n",
      "Epoch 5 - Loss: 0.5380\n",
      "Epoch 5 - Loss: 0.7545\n",
      "Epoch 5 - Loss: 0.9300\n",
      "Epoch 5 - Loss: 0.8576\n",
      "Epoch 5 - Loss: 0.7658\n",
      "Epoch 5 - Loss: 0.6692\n",
      "Epoch 5 - Loss: 0.5447\n",
      "Epoch 5 - Loss: 0.4063\n",
      "Epoch 5 - Loss: 0.9583\n",
      "Epoch 5 - Loss: 0.9464\n",
      "Epoch 5 - Loss: 0.5038\n",
      "Epoch 5 - Loss: 0.6168\n",
      "Epoch 5 - Loss: 0.6620\n",
      "Epoch 5 - Loss: 0.7657\n",
      "Epoch 5 - Loss: 0.4298\n",
      "Epoch 5 - Loss: 0.4843\n",
      "Epoch 5 - Loss: 0.5656\n",
      "Epoch 5 - Loss: 0.4487\n",
      "Epoch 5 - Loss: 0.9939\n",
      "Epoch 5 - Loss: 0.5994\n",
      "Epoch 5 - Loss: 1.1454\n",
      "Epoch 5 - Loss: 0.5611\n",
      "Epoch 5 - Loss: 0.5470\n",
      "Epoch 5 - Loss: 1.1109\n",
      "Epoch 5 - Loss: 0.4086\n",
      "Epoch 5 - Loss: 0.4793\n",
      "Epoch 5 - Loss: 0.6542\n",
      "Epoch 5 - Loss: 0.4143\n",
      "Epoch 5 - Loss: 0.4630\n",
      "Epoch 5 - Loss: 0.8907\n",
      "Epoch 5 - Loss: 0.6738\n",
      "Epoch 5 - Loss: 1.1731\n",
      "Epoch 5 - Loss: 0.8879\n",
      "Epoch 5 - Loss: 1.1635\n",
      "Epoch 5 - Loss: 0.6023\n",
      "Epoch 5 - Loss: 0.4962\n",
      "Epoch 5 - Loss: 0.6023\n",
      "Epoch 5 - Loss: 0.7705\n",
      "Epoch 5 - Loss: 0.7428\n",
      "Epoch 5 - Loss: 0.5588\n",
      "Epoch 5 - Loss: 0.8554\n",
      "Epoch 5 - Loss: 0.5375\n",
      "Epoch 5 - Loss: 1.2380\n",
      "Epoch 5 - Loss: 0.9640\n",
      "Epoch 5 - Loss: 1.6599\n",
      "Epoch 5 - Loss: 0.3816\n",
      "Epoch 5 - Loss: 0.3658\n",
      "Epoch 5 - Loss: 1.0036\n",
      "Epoch 5 - Loss: 0.6545\n",
      "Epoch 5 - Loss: 0.6224\n",
      "Epoch 5 - Loss: 0.6306\n",
      "Epoch 5 - Loss: 0.6836\n",
      "Epoch 5 - Loss: 1.3203\n",
      "Epoch 5 - Loss: 0.3425\n",
      "Epoch 5 - Loss: 0.6327\n",
      "Epoch 5 - Loss: 0.6219\n",
      "Epoch 5 - Loss: 0.5489\n",
      "Epoch 5 - Loss: 0.8675\n",
      "Epoch 5 - Loss: 0.5196\n",
      "Epoch 5 - Loss: 0.6666\n",
      "Epoch 5 - Loss: 0.5731\n",
      "Epoch 5 - Loss: 0.6376\n",
      "Epoch 5 - Loss: 0.5111\n",
      "Epoch 5 - Loss: 0.3819\n",
      "Epoch 5 - Loss: 0.2523\n",
      "Epoch 5 - Loss: 1.1129\n",
      "Epoch 5 - Loss: 0.8155\n",
      "Epoch 5 - Loss: 0.8455\n",
      "Epoch 5 - Loss: 0.6124\n",
      "Epoch 5 - Loss: 0.2371\n",
      "Epoch 5 - Loss: 0.3779\n",
      "Epoch 5 - Loss: 0.4983\n",
      "Epoch 5 - Loss: 0.4640\n",
      "Epoch 5 - Loss: 0.6501\n",
      "Epoch 5 - Loss: 0.5510\n",
      "Epoch 5 - Loss: 0.6340\n",
      "Epoch 5 - Loss: 0.5451\n",
      "Epoch 5 - Loss: 0.4447\n",
      "Epoch 5 - Loss: 0.6341\n",
      "Epoch 5 - Loss: 0.6493\n",
      "Epoch 5 - Loss: 0.2487\n",
      "Epoch 5 - Loss: 0.5966\n",
      "Epoch 5 - Loss: 0.3154\n",
      "Epoch 5 - Loss: 0.7516\n",
      "Epoch 5 - Loss: 0.4935\n",
      "Epoch 5 - Loss: 0.8713\n",
      "Epoch 5 - Loss: 1.0013\n",
      "Epoch 5 - Loss: 1.0860\n",
      "Epoch 5 - Loss: 0.3466\n",
      "Epoch 5 - Loss: 0.6319\n",
      "Epoch 5 - Loss: 0.7953\n",
      "Epoch 5 - Loss: 0.4676\n",
      "Epoch 5 - Loss: 0.3218\n",
      "Epoch 5 - Loss: 0.5334\n",
      "Epoch 5 - Loss: 0.4646\n",
      "Epoch 5 - Loss: 0.7189\n",
      "Epoch 5 - Loss: 0.4241\n",
      "Epoch 5 - Loss: 0.5006\n",
      "Epoch 5 - Loss: 0.7588\n",
      "Epoch 5 - Loss: 0.6737\n",
      "Epoch 5 - Loss: 0.9994\n",
      "Epoch 5 - Loss: 0.7223\n",
      "Epoch 5 - Loss: 0.8180\n",
      "Epoch 5 - Loss: 0.6076\n",
      "Epoch 5 - Loss: 0.4777\n",
      "Epoch 5 - Loss: 0.3164\n",
      "Epoch 5 - Loss: 0.4291\n",
      "Epoch 5 - Loss: 0.7414\n",
      "Epoch 5 - Loss: 0.5546\n",
      "Epoch 5 - Loss: 0.4628\n",
      "Epoch 5 - Loss: 0.6462\n",
      "Epoch 5 - Loss: 0.5785\n",
      "Epoch 5 - Loss: 0.5702\n",
      "Epoch 5 - Loss: 0.4631\n",
      "Epoch 5 - Loss: 0.9559\n",
      "Epoch 5 - Loss: 0.5563\n",
      "Epoch 5 - Loss: 0.3279\n",
      "Epoch 5 - Loss: 1.0668\n",
      "Epoch 5 - Loss: 0.4296\n",
      "Epoch 6 - Loss: 0.4989\n",
      "Epoch 6 - Loss: 0.4738\n",
      "Epoch 6 - Loss: 0.6815\n",
      "Epoch 6 - Loss: 0.3526\n",
      "Epoch 6 - Loss: 0.9191\n",
      "Epoch 6 - Loss: 0.7220\n",
      "Epoch 6 - Loss: 0.7332\n",
      "Epoch 6 - Loss: 0.6913\n",
      "Epoch 6 - Loss: 0.5713\n",
      "Epoch 6 - Loss: 0.7018\n",
      "Epoch 6 - Loss: 0.7033\n",
      "Epoch 6 - Loss: 0.5188\n",
      "Epoch 6 - Loss: 0.7373\n",
      "Epoch 6 - Loss: 0.8908\n",
      "Epoch 6 - Loss: 0.7977\n",
      "Epoch 6 - Loss: 0.7023\n",
      "Epoch 6 - Loss: 0.6397\n",
      "Epoch 6 - Loss: 0.4842\n",
      "Epoch 6 - Loss: 0.3997\n",
      "Epoch 6 - Loss: 0.8222\n",
      "Epoch 6 - Loss: 0.9083\n",
      "Epoch 6 - Loss: 0.4460\n",
      "Epoch 6 - Loss: 0.6336\n",
      "Epoch 6 - Loss: 0.6219\n",
      "Epoch 6 - Loss: 0.7708\n",
      "Epoch 6 - Loss: 0.4261\n",
      "Epoch 6 - Loss: 0.4460\n",
      "Epoch 6 - Loss: 0.5916\n",
      "Epoch 6 - Loss: 0.4100\n",
      "Epoch 6 - Loss: 0.8442\n",
      "Epoch 6 - Loss: 0.5749\n",
      "Epoch 6 - Loss: 0.9598\n",
      "Epoch 6 - Loss: 0.5677\n",
      "Epoch 6 - Loss: 0.5484\n",
      "Epoch 6 - Loss: 1.0602\n",
      "Epoch 6 - Loss: 0.3964\n",
      "Epoch 6 - Loss: 0.4649\n",
      "Epoch 6 - Loss: 0.6066\n",
      "Epoch 6 - Loss: 0.3848\n",
      "Epoch 6 - Loss: 0.4590\n",
      "Epoch 6 - Loss: 0.7662\n",
      "Epoch 6 - Loss: 0.6681\n",
      "Epoch 6 - Loss: 0.9927\n",
      "Epoch 6 - Loss: 0.7820\n",
      "Epoch 6 - Loss: 1.0681\n",
      "Epoch 6 - Loss: 0.5677\n",
      "Epoch 6 - Loss: 0.4629\n",
      "Epoch 6 - Loss: 0.5998\n",
      "Epoch 6 - Loss: 0.7082\n",
      "Epoch 6 - Loss: 0.7138\n",
      "Epoch 6 - Loss: 0.5769\n",
      "Epoch 6 - Loss: 0.8506\n",
      "Epoch 6 - Loss: 0.4910\n",
      "Epoch 6 - Loss: 1.1134\n",
      "Epoch 6 - Loss: 0.8886\n",
      "Epoch 6 - Loss: 1.5275\n",
      "Epoch 6 - Loss: 0.4017\n",
      "Epoch 6 - Loss: 0.3738\n",
      "Epoch 6 - Loss: 0.8904\n",
      "Epoch 6 - Loss: 0.6780\n",
      "Epoch 6 - Loss: 0.5704\n",
      "Epoch 6 - Loss: 0.5920\n",
      "Epoch 6 - Loss: 0.6786\n",
      "Epoch 6 - Loss: 1.1558\n",
      "Epoch 6 - Loss: 0.3238\n",
      "Epoch 6 - Loss: 0.6540\n",
      "Epoch 6 - Loss: 0.6154\n",
      "Epoch 6 - Loss: 0.5286\n",
      "Epoch 6 - Loss: 0.8178\n",
      "Epoch 6 - Loss: 0.4983\n",
      "Epoch 6 - Loss: 0.6485\n",
      "Epoch 6 - Loss: 0.5363\n",
      "Epoch 6 - Loss: 0.5997\n",
      "Epoch 6 - Loss: 0.5086\n",
      "Epoch 6 - Loss: 0.3640\n",
      "Epoch 6 - Loss: 0.2569\n",
      "Epoch 6 - Loss: 0.9778\n",
      "Epoch 6 - Loss: 0.7293\n",
      "Epoch 6 - Loss: 0.7898\n",
      "Epoch 6 - Loss: 0.6329\n",
      "Epoch 6 - Loss: 0.2328\n",
      "Epoch 6 - Loss: 0.3836\n",
      "Epoch 6 - Loss: 0.5199\n",
      "Epoch 6 - Loss: 0.4630\n",
      "Epoch 6 - Loss: 0.6100\n",
      "Epoch 6 - Loss: 0.5183\n",
      "Epoch 6 - Loss: 0.5684\n",
      "Epoch 6 - Loss: 0.5502\n",
      "Epoch 6 - Loss: 0.4213\n",
      "Epoch 6 - Loss: 0.6057\n",
      "Epoch 6 - Loss: 0.5806\n",
      "Epoch 6 - Loss: 0.2318\n",
      "Epoch 6 - Loss: 0.5982\n",
      "Epoch 6 - Loss: 0.3128\n",
      "Epoch 6 - Loss: 0.7525\n",
      "Epoch 6 - Loss: 0.4699\n",
      "Epoch 6 - Loss: 0.8066\n",
      "Epoch 6 - Loss: 0.9311\n",
      "Epoch 6 - Loss: 0.9386\n",
      "Epoch 6 - Loss: 0.3387\n",
      "Epoch 6 - Loss: 0.5805\n",
      "Epoch 6 - Loss: 0.7258\n",
      "Epoch 6 - Loss: 0.4532\n",
      "Epoch 6 - Loss: 0.2967\n",
      "Epoch 6 - Loss: 0.5354\n",
      "Epoch 6 - Loss: 0.4658\n",
      "Epoch 6 - Loss: 0.6329\n",
      "Epoch 6 - Loss: 0.4139\n",
      "Epoch 6 - Loss: 0.4712\n",
      "Epoch 6 - Loss: 0.7536\n",
      "Epoch 6 - Loss: 0.5884\n",
      "Epoch 6 - Loss: 0.6951\n",
      "Epoch 6 - Loss: 0.6911\n",
      "Epoch 6 - Loss: 0.6944\n",
      "Epoch 6 - Loss: 0.6192\n",
      "Epoch 6 - Loss: 0.4811\n",
      "Epoch 6 - Loss: 0.3089\n",
      "Epoch 6 - Loss: 0.3984\n",
      "Epoch 6 - Loss: 0.6364\n",
      "Epoch 6 - Loss: 0.5153\n",
      "Epoch 6 - Loss: 0.4361\n",
      "Epoch 6 - Loss: 0.6855\n",
      "Epoch 6 - Loss: 0.4957\n",
      "Epoch 6 - Loss: 0.5665\n",
      "Epoch 6 - Loss: 0.4586\n",
      "Epoch 6 - Loss: 0.6929\n",
      "Epoch 6 - Loss: 0.4803\n",
      "Epoch 6 - Loss: 0.3142\n",
      "Epoch 6 - Loss: 0.9241\n",
      "Epoch 6 - Loss: 0.4503\n",
      "Epoch 7 - Loss: 0.4991\n",
      "Epoch 7 - Loss: 0.4881\n",
      "Epoch 7 - Loss: 0.6396\n",
      "Epoch 7 - Loss: 0.3145\n",
      "Epoch 7 - Loss: 0.8759\n",
      "Epoch 7 - Loss: 0.6798\n",
      "Epoch 7 - Loss: 0.7176\n",
      "Epoch 7 - Loss: 0.6419\n",
      "Epoch 7 - Loss: 0.5538\n",
      "Epoch 7 - Loss: 0.6544\n",
      "Epoch 7 - Loss: 0.6591\n",
      "Epoch 7 - Loss: 0.5168\n",
      "Epoch 7 - Loss: 0.7032\n",
      "Epoch 7 - Loss: 0.8279\n",
      "Epoch 7 - Loss: 0.7752\n",
      "Epoch 7 - Loss: 0.6681\n",
      "Epoch 7 - Loss: 0.6231\n",
      "Epoch 7 - Loss: 0.3990\n",
      "Epoch 7 - Loss: 0.3947\n",
      "Epoch 7 - Loss: 0.7176\n",
      "Epoch 7 - Loss: 0.8750\n",
      "Epoch 7 - Loss: 0.3980\n",
      "Epoch 7 - Loss: 0.6259\n",
      "Epoch 7 - Loss: 0.6019\n",
      "Epoch 7 - Loss: 0.7433\n",
      "Epoch 7 - Loss: 0.4184\n",
      "Epoch 7 - Loss: 0.4592\n",
      "Epoch 7 - Loss: 0.6059\n",
      "Epoch 7 - Loss: 0.3963\n",
      "Epoch 7 - Loss: 0.7421\n",
      "Epoch 7 - Loss: 0.5663\n",
      "Epoch 7 - Loss: 0.7704\n",
      "Epoch 7 - Loss: 0.5929\n",
      "Epoch 7 - Loss: 0.5374\n",
      "Epoch 7 - Loss: 1.0605\n",
      "Epoch 7 - Loss: 0.3853\n",
      "Epoch 7 - Loss: 0.4481\n",
      "Epoch 7 - Loss: 0.5507\n",
      "Epoch 7 - Loss: 0.3853\n",
      "Epoch 7 - Loss: 0.4505\n",
      "Epoch 7 - Loss: 0.6372\n",
      "Epoch 7 - Loss: 0.6367\n",
      "Epoch 7 - Loss: 0.8714\n",
      "Epoch 7 - Loss: 0.7461\n",
      "Epoch 7 - Loss: 0.9771\n",
      "Epoch 7 - Loss: 0.5199\n",
      "Epoch 7 - Loss: 0.4459\n",
      "Epoch 7 - Loss: 0.6179\n",
      "Epoch 7 - Loss: 0.5951\n",
      "Epoch 7 - Loss: 0.6680\n",
      "Epoch 7 - Loss: 0.5714\n",
      "Epoch 7 - Loss: 0.8333\n",
      "Epoch 7 - Loss: 0.4808\n",
      "Epoch 7 - Loss: 1.0077\n",
      "Epoch 7 - Loss: 0.7536\n",
      "Epoch 7 - Loss: 1.3241\n",
      "Epoch 7 - Loss: 0.4303\n",
      "Epoch 7 - Loss: 0.3987\n",
      "Epoch 7 - Loss: 0.7487\n",
      "Epoch 7 - Loss: 0.7066\n",
      "Epoch 7 - Loss: 0.5037\n",
      "Epoch 7 - Loss: 0.5571\n",
      "Epoch 7 - Loss: 0.6811\n",
      "Epoch 7 - Loss: 0.9149\n",
      "Epoch 7 - Loss: 0.3043\n",
      "Epoch 7 - Loss: 0.6829\n",
      "Epoch 7 - Loss: 0.6340\n",
      "Epoch 7 - Loss: 0.5168\n",
      "Epoch 7 - Loss: 0.8162\n",
      "Epoch 7 - Loss: 0.4661\n",
      "Epoch 7 - Loss: 0.6399\n",
      "Epoch 7 - Loss: 0.4904\n",
      "Epoch 7 - Loss: 0.5883\n",
      "Epoch 7 - Loss: 0.5210\n",
      "Epoch 7 - Loss: 0.3516\n",
      "Epoch 7 - Loss: 0.2589\n",
      "Epoch 7 - Loss: 0.8069\n",
      "Epoch 7 - Loss: 0.6790\n",
      "Epoch 7 - Loss: 0.7418\n",
      "Epoch 7 - Loss: 0.6039\n",
      "Epoch 7 - Loss: 0.2398\n",
      "Epoch 7 - Loss: 0.3993\n",
      "Epoch 7 - Loss: 0.5414\n",
      "Epoch 7 - Loss: 0.4552\n",
      "Epoch 7 - Loss: 0.6132\n",
      "Epoch 7 - Loss: 0.5295\n",
      "Epoch 7 - Loss: 0.5310\n",
      "Epoch 7 - Loss: 0.5311\n",
      "Epoch 7 - Loss: 0.4168\n",
      "Epoch 7 - Loss: 0.5962\n",
      "Epoch 7 - Loss: 0.4568\n",
      "Epoch 7 - Loss: 0.2161\n",
      "Epoch 7 - Loss: 0.5588\n",
      "Epoch 7 - Loss: 0.3241\n",
      "Epoch 7 - Loss: 0.7196\n",
      "Epoch 7 - Loss: 0.4863\n",
      "Epoch 7 - Loss: 0.6897\n",
      "Epoch 7 - Loss: 0.8643\n",
      "Epoch 7 - Loss: 0.8166\n",
      "Epoch 7 - Loss: 0.3135\n",
      "Epoch 7 - Loss: 0.5303\n",
      "Epoch 7 - Loss: 0.6921\n",
      "Epoch 7 - Loss: 0.5119\n",
      "Epoch 7 - Loss: 0.2528\n",
      "Epoch 7 - Loss: 0.5432\n",
      "Epoch 7 - Loss: 0.4675\n",
      "Epoch 7 - Loss: 0.5659\n",
      "Epoch 7 - Loss: 0.4132\n",
      "Epoch 7 - Loss: 0.3985\n",
      "Epoch 7 - Loss: 0.7544\n",
      "Epoch 7 - Loss: 0.5531\n",
      "Epoch 7 - Loss: 0.5420\n",
      "Epoch 7 - Loss: 0.7332\n",
      "Epoch 7 - Loss: 0.6076\n",
      "Epoch 7 - Loss: 0.7127\n",
      "Epoch 7 - Loss: 0.4936\n",
      "Epoch 7 - Loss: 0.3309\n",
      "Epoch 7 - Loss: 0.4090\n",
      "Epoch 7 - Loss: 0.5471\n",
      "Epoch 7 - Loss: 0.4720\n",
      "Epoch 7 - Loss: 0.4007\n",
      "Epoch 7 - Loss: 0.6659\n",
      "Epoch 7 - Loss: 0.4898\n",
      "Epoch 7 - Loss: 0.5485\n",
      "Epoch 7 - Loss: 0.4311\n",
      "Epoch 7 - Loss: 0.4539\n",
      "Epoch 7 - Loss: 0.4032\n",
      "Epoch 7 - Loss: 0.3067\n",
      "Epoch 7 - Loss: 0.8119\n",
      "Epoch 7 - Loss: 0.4329\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === Optimizer ===\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        label_ids = batch[\"label_ids\"].to(device)\n",
    "\n",
    "        full_input_ids = torch.cat([input_ids, label_ids], dim=1)\n",
    "        full_attention_mask = torch.cat([attention_mask, torch.ones_like(label_ids)], dim=1)\n",
    "\n",
    "        # Mask labels: only compute loss on label portion\n",
    "        labels = torch.full_like(full_input_ids, -100)\n",
    "        labels[:, input_ids.shape[1]:] = label_ids\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=full_input_ids,\n",
    "            attention_mask=full_attention_mask, \n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk_p04_r00_v05_c05_middle.jpg --  The person is standing in the living room, holding a Wii remote and playing a video game.<|eot_id|>\n",
      "Sitdown_p12_r01_v03_c04_middle.jpg --  The person in the image is sitting on a green couch, holding a white remote control in their hands\n",
      "Walk_p10_r05_v13_c07_middle.jpg --  The image is blurry, but it appears to show a person standing at a counter. The person is\n",
      "Sitdown_p09_r03_v07_c04_middle.jpg --  The person in this image is sitting on a green couch, possibly reading or looking down at a book\n",
      "Sitdown_p10_r04_v03_c04_middle.jpg --  The person in the image appears to be sitting on a green couch, leaning forward with their hands on\n",
      "Drink.Frombottle_p06_r01_v10_c02_middle.jpg --  the person in the image appears to be sitting at a table with a white tablecloth, eating a\n",
      "Laydown_p03_r00_v19_c05_middle.jpg --  The person in the image is sitting on a green couch, watching a football game on the television.\n",
      "Walk_p04_r02_v07_c05_middle.jpg --  The person in the image is standing in the corner of the room, wearing a striped shirt and white\n",
      "Sitdown_p25_r01_v02_c05_middle.jpg --  A person is sitting on a couch in a living room, watching a television.<|eot_id|>\n",
      "Sitdown_p10_r11_v10_c01_middle.jpg --  The person in the image is an elderly woman with short white hair, standing at a table and looking\n",
      "Drink.Frombottle_p03_r05_v05_c05_middle.jpg --  The person in the image is standing in the middle of a living room, holding a remote control in\n",
      "Drink.Frombottle_p12_r01_v06_c05_middle.jpg --  The person in the image is standing in a living room, holding a Wii remote in their right hand\n",
      "Laydown_p19_r00_v03_c04_middle.jpg --  The person in the image appears to be asleep on a green couch. They are wearing an orange t\n",
      "Walk_p20_r04_v15_c07_middle.jpg --  The person is standing in a kitchen, with their back to the camera. They are holding a green\n",
      "Walk_p03_r08_v02_c05_middle.jpg --  The person in the image appears to be walking, as suggested by their forward-facing stance and the blurred\n",
      "Laydown_p18_r01_v03_c05_middle.jpg --  A person is sitting on a green sofa. The person is wearing a white shirt with a colorful design\n",
      "Sitdown_p10_r03_v10_c01_middle.jpg --  The person in the image appears to be sitting at a table with a drink in their hand, possibly\n",
      "Sitdown_p14_r02_v10_c01_middle.jpg --  A person sitting at a table, with a cup of coffee and a plate of food in front of\n",
      "Drink.Frombottle_p04_r02_v04_c04_middle.jpg --  The person in the image appears to be standing in a living room, holding a white object in their\n",
      "Laydown_p16_r00_v05_c05_middle.jpg --  the person is sitting on a couch, with their back to the camera, and appears to be playing\n",
      "Walk_p14_r07_v15_c07_middle.jpg --  The man in the image is walking past a kitchen counter and a mirror. He is wearing a striped\n",
      "Drink.Frombottle_p18_r07_v05_c05_middle.jpg --  The person in the image appears to be sitting on a green couch watching something on the TV. The\n",
      "Drink.Frombottle_p12_r03_v04_c05_middle.jpg --  The person is sitting on a green couch, facing a TV. They are wearing a red shirt and\n",
      "Sitdown_p17_r05_v08_c02_middle.jpg --  The person in the image appears to be seated in front of a table, possibly working on a laptop\n",
      "Sitdown_p20_r03_v03_c04_middle.jpg --  The person is sitting on a couch, with a tablet on their lap. They are in a living\n",
      "Laydown_p09_r00_v02_c04_middle.jpg --  A person is lying on a couch. The person is wearing a white and black plaid shirt and\n",
      "Walk_p03_r01_v13_c02_middle.jpg --  The person in the image appears to be walking through a room, possibly in the process of leaving.\n",
      "Sitdown_p14_r19_v05_c05_middle.jpg --  The person is sitting on the sofa and watching a television. The person is watching a television program,\n",
      "Sitdown_p10_r01_v09_c01_middle.jpg --  The person in the image appears to be an older woman with white hair and wearing a teal shirt,\n",
      "Drink.Frombottle_p06_r04_v10_c01_middle.jpg --  the person is sitting at a table with a red folder and a cup of coffee. she appears to\n",
      "Laydown_p15_r02_v02_c05_middle.jpg --  A person is reclined on a green couch in the living room.<|eot_id|>\n",
      "Sitdown_p19_r02_v07_c01_middle.jpg --  A person is sitting at a table, holding a drink and a magazine. They are wearing an orange\n",
      "Walk_p15_r05_v13_c07_middle.jpg --  The man in the image is walking through a kitchen. He appears to be wearing a gray shirt with\n",
      "Sitdown_p17_r12_v02_c05_middle.jpg --  A man is sitting on a couch in a living room, watching a television. He is holding a\n",
      "Drink.Frombottle_p06_r03_v10_c02_middle.jpg --  The person in the image is sitting at a table drinking from a bottle. The person is wearing a\n",
      "Drink.Frombottle_p06_r03_v06_c05_middle.jpg --  A person is sitting on a green couch in a living room, holding a white remote control and watching\n",
      "Laydown_p07_r01_v03_c05_middle.jpg --  The person is sitting on a green couch with a gray blanket draped over the back, watching television.\n",
      "Laydown_p14_r06_v02_c05_middle.jpg --  A person is lying on a sofa in the living room. The person is wearing a white shirt,\n",
      "Sitdown_p07_r00_v03_c05_middle.jpg --  The person in this image is sitting on a couch with a video game controller in their hand, likely\n",
      "Drink.Frombottle_p04_r04_v04_c05_middle.jpg --  The person is standing in a living room, wearing a black and white striped shirt and khaki pants\n",
      "Laydown_p02_r01_v07_c04_middle.jpg --  A person is sitting on a green couch, holding a blanket. The person has short grey hair and\n",
      "Sitdown_p04_r00_v01_c05_middle.jpg --  the person is sitting on a couch in a living room, holding a Wii remote in their right hand\n",
      "Walk_p16_r02_v02_c05_middle.jpg --  The person in the image is standing in the middle of the room, with their left hand raised to\n",
      "Walk_p15_r19_v13_c06_middle.jpg --  A man is standing in a kitchen, holding a pan, and appears to be preparing food. The\n",
      "Walk_p25_r29_v05_c05_middle.jpg --  The person is standing in the middle of a living room, possibly holding an object, with their back\n",
      "Drink.Frombottle_p12_r01_v07_c05_middle.jpg --  The person is standing in a living room, with their arms raised as if they are playing a video\n",
      "Sitdown_p17_r11_v04_c04_middle.jpg --  The person in this image is sitting on a green couch, holding a tablet in their lap. The\n",
      "Drink.Frombottle_p12_r01_v02_c04_middle.jpg --  A woman is sitting on a green couch, holding a remote control in her hands.<|eot_id|>\n",
      "Walk_p20_r11_v13_c03_middle.jpg --  A person is standing in the background of a kitchen, holding a coffee pot. It looks like they\n",
      "Laydown_p09_r00_v06_c05_middle.jpg --  The person in the image is lying on a green couch, with their legs propped up on a\n",
      "Drink.Frombottle_p16_r00_v15_c03_middle.jpg --  The person is standing in a kitchen, holding a cup in their hand and appears to be drinking from\n",
      "Drink.Frombottle_p13_r00_v13_c01_middle.jpg --  the person is standing in the office, leaning over a table to take a picture of it.<|eot_id|>\n",
      "Walk_p16_r01_v14_c03_middle.jpg --  The person is standing in a kitchen, with their back to the camera. They are wearing a sleeve\n",
      "Walk_p03_r45_v04_c04_middle.jpg --  The person is standing in a living room and appears to be reaching out towards the couch.<|eot_id|>\n",
      "Sitdown_p17_r05_v09_c02_middle.jpg --  The person in the image appears to be sitting at a table with a laptop in front of them,\n",
      "Sitdown_p10_r09_v10_c01_middle.jpg --  A person is sitting at a table, looking down at something in their hands.<|eot_id|>\n",
      "Drink.Frombottle_p13_r01_v13_c02_middle.jpg --  The image shows a woman standing in a dining room. She is wearing a white shirt and black pants\n",
      "Drink.Frombottle_p06_r01_v10_c01_middle.jpg --  The person in this image is sitting at a table and appears to be eating something. She is wearing\n",
      "Sitdown_p14_r01_v10_c01_middle.jpg --  The person in the image appears to be sitting at a table in a room with a white table and\n",
      "Laydown_p03_r02_v06_c05_middle.jpg --  the person is sitting on the couch with their feet up and watching TV. They are wearing a pl\n",
      "Drink.Frombottle_p03_r13_v03_c05_middle.jpg --  The person is sitting on a couch, facing the television. He appears to be watching it, possibly\n",
      "Walk_p12_r18_v04_c05_middle.jpg --  the person is standing in the center of the room, looking down at the floor.<|eot_id|>\n",
      "Sitdown_p03_r00_v12_c02_middle.jpg --  The man in the image is standing behind a table, reaching to open a magazine that is lying on\n",
      "Drink.Frombottle_p15_r00_v04_c05_middle.jpg --  The man is standing in the middle of the room, looking at the TV. He is wearing a\n",
      "Drink.Frombottle_p13_r00_v06_c04_middle.jpg --  A woman is sitting on a couch, drinking from a cup. She is wearing a white shirt and\n",
      "Walk_p11_r00_v06_c04_middle.jpg --  A man is standing in the background of a living room, possibly looking at the camera. He is\n",
      "Walk_p06_r08_v13_c03_middle.jpg --  The image shows a person walking down a long, narrow kitchen space. \n",
      "\n",
      "The person is walking away\n",
      "Laydown_p19_r00_v04_c05_middle.jpg --  The person is sitting on a couch watching a television.<|eot_id|>\n",
      "Sitdown_p25_r06_v10_c01_middle.jpg --  The person in the image is seated at a long, rectangular table, with their hands resting on the\n",
      "Drink.Frombottle_p04_r13_v04_c05_middle.jpg --  The person in the image is sitting on a couch, watching a sports game on TV.<|eot_id|>\n",
      "Laydown_p09_r01_v02_c04_middle.jpg --  A person is lying on a green couch, watching a television, with their feet propped up on\n",
      "Sitdown_p15_r01_v12_c01_middle.jpg --  The person in the image appears to be sitting at a table, with their right hand resting on their\n",
      "Walk_p25_r02_v14_c07_middle.jpg --  The person in the image appears to be a woman, she is wearing a white dress with a floral\n",
      "Sitdown_p07_r02_v06_c05_middle.jpg --  The person in the image is sitting on a couch, facing a television. They are holding a game\n",
      "Drink.Frombottle_p04_r06_v04_c05_middle.jpg --  The person in the image is standing in a living room, holding a controller and appears to be playing\n",
      "Drink.Frombottle_p15_r00_v05_c04_middle.jpg --  The person in the image is drinking from a glass of water, with their right hand raised and holding\n",
      "Drink.Frombottle_p06_r02_v06_c04_middle.jpg --  The person in the image is sitting on a couch and drinking from a glass. There are books and\n",
      "Sitdown_p14_r02_v07_c04_middle.jpg --  The person is sitting on a green couch, with their hands on their lap. They are wearing a\n",
      "Laydown_p20_r00_v04_c05_middle.jpg --  The person is sitting on a couch with a blanket over their legs. Their arms are raised in the\n",
      "Laydown_p18_r00_v06_c04_middle.jpg --  A man is lying on a green couch, holding a remote control in his right hand, with his\n",
      "Drink.Frombottle_p03_r04_v05_c05_middle.jpg --  The person is standing in the living room, holding a drink in their hand. They are wearing a\n",
      "Walk_p03_r07_v02_c05_middle.jpg --  The person is standing in the living room, looking down at something on the floor. They are wearing\n",
      "Walk_p15_r00_v02_c05_middle.jpg --  A man is standing in a living room, with his back to the camera, and appears to be\n",
      "Sitdown_p17_r02_v04_c04_middle.jpg --  The person in this image is sitting on a green couch, holding a phone to their ear and talking\n",
      "Laydown_p25_r00_v03_c05_middle.jpg --  The person in this image is sitting on a couch in a living room, watching a TV. They\n",
      "Drink.Frombottle_p15_r00_v16_c03_middle.jpg --  The person in the image is sitting in front of a computer, and appears to be working. They\n",
      "Sitdown_p10_r06_v05_c05_middle.jpg --  A man is sitting on a couch in a living room, looking at a TV. He is wearing\n",
      "Laydown_p17_r05_v05_c05_middle.jpg --  A person is sitting on a couch in a living room, watching a TV. The person is sitting\n",
      "Sitdown_p07_r04_v05_c05_middle.jpg --  The person in this image is sitting on a green couch, reading a book. The person appears to\n",
      "Walk_p15_r12_v17_c03_middle.jpg --  The person in the image appears to be doing the dishes. He is standing at a sink and appears\n",
      "Laydown_p11_r02_v02_c04_middle.jpg --  The person in the image appears to be sitting on a couch in a living room, possibly watching TV\n",
      "Drink.Frombottle_p15_r00_v11_c01_middle.jpg --  A man is holding a water bottle in his left hand, standing in an office with a white table\n",
      "Drink.Frombottle_p06_r00_v06_c05_middle.jpg --  The person in this image is sitting on a couch, holding a Nintendo Wii controller in both hands.\n",
      "Laydown_p25_r03_v05_c05_middle.jpg --  The person is sitting on a green couch watching a television in a living room.<|eot_id|>\n",
      "Sitdown_p25_r04_v06_c04_middle.jpg --  A woman is sitting on a green couch in a living room. She is wearing white shorts and a\n",
      "Walk_p12_r14_v05_c05_middle.jpg --  The person in this image appears to be standing in a living room with a remote control in their hand\n",
      "Walk_p19_r04_v09_c02_middle.jpg --  The person in the image appears to be an older gentleman, wearing a red shirt and black pants.\n",
      "Sitdown_p11_r01_v09_c01_middle.jpg --  The person in the image is sitting at a desk, facing a laptop. Their hands are raised in\n",
      "Drink.Frombottle_p14_r00_v06_c05_middle.jpg --  A person is sitting on a green couch in a living room, watching a television. \n",
      "The person\n",
      "Laydown_p20_r00_v04_c04_middle.jpg --  The person in the image appears to be sitting on a couch, possibly in a living room setting.\n",
      "Laydown_p25_r00_v03_c04_middle.jpg --  The person in the image is reclined on a green couch in a living room, with their head\n",
      "Laydown_p18_r01_v06_c04_middle.jpg --  The person in the image is sitting on a green couch with their legs crossed, with a blanket on\n",
      "Drink.Frombottle_p16_r01_v15_c03_middle.jpg --  The person in the image appears to be in a kitchen, standing in front of a sink, holding\n",
      "Sitdown_p18_r02_v05_c05_middle.jpg --  The person in the image is sitting on a couch with their back to the camera. They are wearing\n",
      "Walk_p19_r12_v03_c05_middle.jpg --  The person in the image is standing in a living room, facing a television screen. They are wearing\n",
      "Walk_p25_r24_v04_c05_middle.jpg --  The person in the image appears to be standing in a living room, holding an object in their hands\n",
      "Drink.Frombottle_p03_r06_v03_c05_middle.jpg --  A man is sitting on a green couch watching a tv, he is sitting on the couch facing a\n",
      "Laydown_p25_r03_v03_c04_middle.jpg --  The person is lying on a green sofa, leaning against a pile of pillows, with their head resting\n",
      "Laydown_p02_r02_v07_c05_middle.jpg --  A person is lying on a couch with a blanket on their body, with their head resting on a\n",
      "Drink.Frombottle_p12_r03_v02_c05_middle.jpg --  The person is sitting on a couch in a living room, watching a television.<|eot_id|>\n",
      "Walk_p16_r36_v12_c06_middle.jpg --  The person in the image appears to be standing in a kitchen, with their back turned to the camera\n",
      "Walk_p10_r11_v12_c03_middle.jpg --  A woman is standing in a kitchen, holding a large metal pot with both hands. She is wearing\n",
      "Drink.Frombottle_p03_r01_v03_c04_middle.jpg --  The person in this image is sitting on a green couch and drinking a beverage from a clear cup.\n",
      "Sitdown_p14_r03_v02_c05_middle.jpg --  A person is sitting on a couch in a living room, with their back to the camera, holding\n",
      "Drink.Frombottle_p14_r02_v06_c05_middle.jpg --  The person is sitting on a green couch, watching a television that is mounted on the wall. The\n",
      "Drink.Frombottle_p04_r10_v04_c04_middle.jpg --  The person in the image is standing in a living room, holding a Wii remote in their right hand\n",
      "Laydown_p15_r01_v02_c04_middle.jpg --  The person in the image is lying on a couch, with their legs raised and arms outstretched\n",
      "Walk_p17_r00_v12_c03_middle.jpg --  The person in the image is standing in a kitchen, possibly a commercial kitchen, as indicated by the\n",
      "Sitdown_p16_r03_v07_c01_middle.jpg --  A woman sitting at a table with a laptop in front of her, likely working or waiting for something\n",
      "Sitdown_p04_r05_v03_c05_middle.jpg --  the person is sitting on a green couch and is playing a video game. there is a television in\n",
      "Drink.Frombottle_p03_r02_v05_c04_middle.jpg --  The person in the image is sitting on a green couch, drinking from a glass. He is wearing\n",
      "Sitdown_p20_r04_v07_c04_middle.jpg --  The person in the image is sitting on a green couch, holding a white object in their hands.\n",
      "Drink.Frombottle_p14_r00_v13_c02_middle.jpg --  The image shows a man sitting at a table, with his head in his hands, appearing to be\n",
      "Walk_p06_r00_v05_c04_middle.jpg --  A woman is standing in the image, and appears to be walking away from the camera. She is\n",
      "Walk_p16_r28_v12_c06_middle.jpg --  The image shows a woman in a kitchen. She is facing away from the camera, and appears to\n",
      "Sitdown_p14_r05_v02_c04_middle.jpg --  The person in the image is sitting on a green couch, holding a laptop computer. They appear to\n",
      "Laydown_p10_r02_v03_c04_middle.jpg --  The person in the image appears to be sleeping on a couch. They are covered with a blanket and\n",
      "Drink.Frombottle_p09_r00_v07_c04_middle.jpg --  The person in this image is sitting on a green couch, drinking a beverage from a clear glass.\n",
      "Drink.Frombottle_p04_r00_v07_c04_middle.jpg --  A person is sitting on a green couch in a living room, holding a remote control. They are\n",
      "Walk_p25_r04_v14_c03_middle.jpg --  The person in the image appears to be walking through a kitchen area. The person appears to be wearing\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "generated=[]\n",
    "\"Abid/proj_with_marco/Data/train_data\"\n",
    "test_data=get_image_files(\"/home/coder/Abid/proj_with_marco/Data/train_data\")\n",
    "for image in test_data:\n",
    "    pil_image = Image.open(\"/home/coder/Abid/proj_with_marco/Data/train_data/\"+image).convert(\"RGB\")\n",
    "\n",
    "    # Define your text prompt\n",
    "    #prompt =\"<|image|>what is the person doing?\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        inputs = processor(text=prompt, images=pil_image, return_tensors=\"pt\").to(device) \n",
    "        outputs=model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens= 20,\n",
    "            min_length= 20,\n",
    "            do_sample= True,\n",
    "            no_repeat_ngram_size = 4\n",
    "        )\n",
    "        generated_text = processor.decode(outputs[0], skip_special_tokens=False)\n",
    "    generated.append(f\"{image} -- {generated_text.split('description:')[1]}\")\n",
    "    print(f\"{image} -- {generated_text.split('description:')[1]}\")\n",
    "    \n",
    "with open('noFinetune.json','w') as f:\n",
    "    json.dump(generated,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 Images, from idx 0 to 129\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pprint\n",
    "\n",
    "def read_img_folder(folder='../Data/train_data',suffix='_middle.jpg'):\n",
    "    imgs = [img.replace(suffix,'') for img in os.listdir(folder)]\n",
    "    print(f\"{len(imgs)} Images, from idx 0 to {len(imgs)-1}\")\n",
    "    return imgs\n",
    "\n",
    "def show_img_and_caps(imgs,folder='../Data/train_data',suffix='_middle.jpg',idx=0):\n",
    "    print(imgs[idx])\n",
    "    pic = Image.open(folder+\"/\"+imgs[idx]+suffix)\n",
    "    pic.show()\n",
    "    pprint.pprint(caps[imgs[idx]])\n",
    "imgs = read_img_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Drink_Frombottle',\n",
       " 'The man is sitting on the sofa and drinking from a glass. \\n    The man is sitting on the sofa and drinking from a glass. \\n    The man is sitting on the sofa and drinking from a glass. \\n    The man is sitting on',\n",
       " 'The person is sitting on the couch and drinking from a cup.',\n",
       " 'the person is drinking water from a bottle',\n",
       " 'The person is sitting on the person is drinking.',\n",
       " 'The person is sitting on a couch and drinking from a bottle.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show_img_and_caps(imgs,idx=6)\n",
    "caps['Drink_Frombottle_p03_r01_v03_c04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 image overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - Loss: 10.3623\n",
      "Step 1 - Loss: 4.4733\n",
      "Step 2 - Loss: 7.8711\n",
      "Step 3 - Loss: 5.2411\n",
      "Step 4 - Loss: 6.9303\n",
      "Step 5 - Loss: 6.3600\n",
      "Step 6 - Loss: 28.7334\n",
      "Step 7 - Loss: 10.7192\n",
      "Step 8 - Loss: 7.8417\n",
      "Step 9 - Loss: 6.2502\n",
      "Step 10 - Loss: 6.5285\n",
      "Step 11 - Loss: 4.9882\n",
      "Step 12 - Loss: 4.6794\n",
      "Step 13 - Loss: 4.5411\n",
      "Step 14 - Loss: 3.7174\n",
      "Step 15 - Loss: 24.9111\n",
      "Step 16 - Loss: 3.7913\n",
      "Step 17 - Loss: 3.8505\n",
      "Step 18 - Loss: 4.0146\n",
      "Step 19 - Loss: 3.7713\n",
      "Step 20 - Loss: 3.5547\n",
      "Step 21 - Loss: 3.4515\n",
      "Step 22 - Loss: 3.1453\n",
      "Step 23 - Loss: 2.9508\n",
      "Step 24 - Loss: 2.8847\n",
      "Step 25 - Loss: 2.7766\n",
      "Step 26 - Loss: 2.6599\n",
      "Step 27 - Loss: 2.4898\n",
      "Step 28 - Loss: 2.3936\n",
      "Step 29 - Loss: 2.3410\n",
      "Step 30 - Loss: 2.0899\n",
      "Step 31 - Loss: 2.0051\n",
      "Step 32 - Loss: 1.9911\n",
      "Step 33 - Loss: 1.9856\n",
      "Step 34 - Loss: 1.9323\n",
      "Step 35 - Loss: 1.9026\n",
      "Step 36 - Loss: 1.8333\n",
      "Step 37 - Loss: 1.7020\n",
      "Step 38 - Loss: 1.6065\n",
      "Step 39 - Loss: 1.5376\n",
      "Step 40 - Loss: 1.4488\n",
      "Step 41 - Loss: 1.3601\n",
      "Step 42 - Loss: 1.2625\n",
      "Step 43 - Loss: 1.2064\n",
      "Step 44 - Loss: 1.0791\n",
      "Step 45 - Loss: 0.9530\n",
      "Step 46 - Loss: 0.8228\n",
      "Step 47 - Loss: 0.7135\n",
      "Step 48 - Loss: 0.6089\n",
      "Step 49 - Loss: 0.4926\n",
      "Step 50 - Loss: 0.4057\n",
      "Step 51 - Loss: 0.2900\n",
      "Step 52 - Loss: 0.2540\n",
      "Step 53 - Loss: 0.1539\n",
      "Step 54 - Loss: 0.1181\n",
      "Step 55 - Loss: 0.0651\n",
      "Step 56 - Loss: 0.0372\n",
      "Step 57 - Loss: 0.0202\n",
      "Step 58 - Loss: 0.0178\n",
      "Step 59 - Loss: 0.0109\n",
      "Step 60 - Loss: 0.0073\n",
      "✅ Overfitting successful.\n",
      "Generated Text: <|begin_of_text|><|image|><|begin_of_text|> what is the person doing?<|begin_of_text|>the person is watching tv <|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# === Single example ===\n",
    "image = Image.open(\"tv.jpg\").convert(\"RGB\")\n",
    "prompt = \"what is the person doing?\"\n",
    "label = \"the person is watching tv <|end_of_text|> \"\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)  # ✅ correct\n",
    "target_ids = processor.tokenizer(label, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Combine input + target\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "full_input_ids = torch.cat([input_ids, target_ids], dim=1)\n",
    "full_attention_mask = torch.cat([attention_mask, torch.ones_like(target_ids)], dim=1)\n",
    "\n",
    "# Mask loss: only compute over label\n",
    "labels = torch.full_like(full_input_ids, -100)\n",
    "labels[:, input_ids.shape[1]:] = target_ids\n",
    "\n",
    "# === Optimizer ===\n",
    "optimizer = AdamW(model.parameters(), lr=5e-3)\n",
    "\n",
    "# === Overfit Loop ===\n",
    "for step in range(100):\n",
    "    outputs = model(\n",
    "        input_ids=full_input_ids,\n",
    "        attention_mask=full_attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Step {step} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Stop early if loss is very low\n",
    "    if loss.item() < 0.01:\n",
    "        print(\"✅ Overfitting successful.\")\n",
    "        break\n",
    "\n",
    "\n",
    "image_path = \"tv.jpg\"  # Replace with your actual image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Define your text prompt\n",
    "prompt =\"<|image|> what is the person doing ?\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device) \n",
    "    outputs=model.generate(**inputs,max_length=30)\n",
    "    generated_text = processor.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gouga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
